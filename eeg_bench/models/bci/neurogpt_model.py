from ..abstract_model import AbstractModel
from typing import List, Dict, cast, Literal
import numpy as np
import argparse
import torch
import os
import random
import numpy as np
import pandas as pd
import time
import datetime
import json
from mne.io import BaseRaw
from torch import manual_seed
from .NeuroGPT.src.encoder.conformer_braindecode import EEGConformer
from .NeuroGPT.src.decoder.make_decoder import make_decoder
from .NeuroGPT.src.embedder.make import make_embedder
from .NeuroGPT.src.decoder.unembedder import make_unembedder
from .NeuroGPT.src.trainer.make import make_trainer
from .NeuroGPT.src.utils import cv_split_bci
from .NeuroGPT.src.trainer.base import Trainer
from .NeuroGPT.src.model import Model
from sklearn.model_selection import train_test_split
import logging
from .NeuroGPT.src.utils_2 import NeuroGPTDataset2
from .LaBraM.utils_2 import calc_class_weights, reverse_map_label, n_unique_labels
import os
import requests
from pathlib import Path
from ...config import get_config_value

def check_and_download_pretrained_model():
    chkpt_dir = Path(get_config_value("chkpt"))
    if not os.path.exists(chkpt_dir):
        os.makedirs(chkpt_dir, exist_ok=True)
    encoder_path = chkpt_dir / "pytorch_model.bin"
    if not os.path.exists(encoder_path):
        print("Neuro-GPT pretrained_model file not found. Downloading pytorch_model.bin ...")
        url = "https://huggingface.co/wenhuic/Neuro-GPT/resolve/main/pretrained_model/pytorch_model.bin"
        response = requests.get(url, stream=True)
        os.makedirs(os.path.dirname(encoder_path), exist_ok=True)
        with open(encoder_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
    return encoder_path

def make_model(model_config: Dict=None, class_weights: List[float]=None) -> Model:
    """Make model from model_config 
    (as generated by get_config()).
    """
    chann_coords = None
    
    encoder = EEGConformer(n_outputs=model_config["num_decoding_classes"], n_chans=22, n_times=model_config['chunk_len'], ch_pos=chann_coords, is_decoding_mode=model_config["ft_only_encoder"])
    #calculates the output dimension of the encoder, which is the output of transformer layer.
    model_config["parcellation_dim"] = ((model_config['chunk_len'] - model_config['filter_time_length'] + 1 - model_config['pool_time_length']) // model_config['stride_avg_pool'] + 1) * model_config['n_filters_time']

    embedder = make_embedder(
        training_style=model_config["training_style"],
        architecture=model_config["architecture"],
        in_dim=model_config["parcellation_dim"], # flattened, channel x chunk length
        embed_dim=model_config["embedding_dim"],
        num_hidden_layers=model_config["num_hidden_layers_embedding_model"],
        dropout=model_config["dropout"],
        n_positions=model_config["n_positions"]
    )
    decoder = make_decoder(
        architecture=model_config["architecture"],
        num_hidden_layers=model_config["num_hidden_layers"],
        embed_dim=model_config["embedding_dim"],
        num_attention_heads=model_config["num_attention_heads"],
        n_positions=model_config["n_positions"],
        intermediate_dim_factor=model_config["intermediate_dim_factor"],
        hidden_activation=model_config["hidden_activation"],
        dropout=model_config["dropout"]
    )

    if model_config["embedding_dim"] != model_config["parcellation_dim"]:
        unembedder = make_unembedder(
            embed_dim=model_config["embedding_dim"],
            num_hidden_layers=model_config["num_hidden_layers_unembedding_model"],
            out_dim=model_config["parcellation_dim"],
            dropout=model_config["dropout"],
        )
    else:
        print("No Embedder and Unembedder!")
        unembedder = None

    model = Model(
        encoder=encoder,
        embedder=embedder,
        decoder=decoder,
        unembedder=unembedder
    )

    model.switch_ft_mode(ft_encoder_only=True)

    model.switch_decoding_mode(
        is_decoding_mode=True,
        num_decoding_classes=model_config["num_decoding_classes"]
    )

    model.from_pretrained(model_config["pretrained_model"])

    if model_config["freeze_embedder"]:
        for param in model.embedder.parameters():
            param.requires_grad = False

    if model_config["freeze_decoder"]:
        for param in model.decoder.parameters():
            param.requires_grad = False

    if model_config["freeze_encoder"]:
        for name, param in model.encoder.named_parameters():
            if 'fc.' in name \
            or 'final_layer' in name:
                continue
            else:
                param.requires_grad = False

    if 'freeze_decoder_without_pooler_heads' in model_config \
        and model_config["freeze_decoder_without_pooler_heads"]:
        for name, param in model.decoder.named_parameters():
            if 'pooler_layer' in name \
            or 'decoding_head' in name \
            or 'is_next_head' in name:
                continue
            else:
                param.requires_grad = False

    if model_config["freeze_unembedder"] and unembedder is not None:
        for param in model.unembedder.parameters():
            param.requires_grad = False

    if class_weights is not None:
        model.embedder.xe_loss = torch.nn.CrossEntropyLoss(reduction='mean', weight=class_weights)
    
    return model


class NeuroGPTModel(AbstractModel):
    def __init__(
        self,
        seed: int = 42,
        freeze_encoder: bool = False,
    ):
        super().__init__("NeuroGPTModel")
        assert torch.cuda.is_available(), "CUDA is not available"
        logging.info("Initializing NeuroGPTModel")

        self.freeze_encoder = freeze_encoder
        self.config = {
            "training_style": "decoding",
            "num_decoding_classes": 2,
            "training_steps": 25000,
            "eval_every_n_steps": 1000,
            "log_every_n_steps": 500,
            "num_chunks": 2,
            "per_device_training_batch_size": 32,
            "per_device_validation_batch_size": 32,
            "chunk_len": 500,
            "chunk_ovlp": 0,
            "run_name": "dst",
            "ft_only_encoder": True,
            "fold_i": 0,
            "num_encoder_layers": 6,
            "num_hidden_layers": 6,
            "learning_rate": 1e-5,
            "use_encoder": True,
            "embedding_dim": 1024,
            "pretrained_model": check_and_download_pretrained_model(),
            "dst_data_path": "./models/NeuroGPT/bci2a_eeg_npz/",
            # Additional parameters with reasonable defaults:
            "do_train": True,
            "resume_from": None,
            "architecture": "GPT",
            "num_hidden_layers_embedding_model": 1,
            "freeze_embedder": False,
            "num_hidden_layers_unembedding_model": 1,
            "freeze_unembedder": False,
            "num_attention_heads": 16,  # 1024 // 64
            "intermediate_dim_factor": 4,
            "hidden_activation": "gelu_new",
            "freeze_decoder": False,
            "freeze_decoder_without_pooler_heads": False,
            "freeze_encoder": self.freeze_encoder,
            "log_dir": "results/models/upstream",
            "wandb_mode": "disabled",
            "wandb_project_name": "learning-from-brains",
            "seed": seed,
            "set_seed": True,
            "fp16": True,
            "deepspeed": None,
            "local_rank": -1,
            "num_workers": 2,
            "plot_model_graph": False,
            "smoke_test": False,
            "bold_dummy_mode": False,
            "do_normalization": True,
            "filter_time_length": 25,
            "pool_time_length": 75,
            "stride_avg_pool": 15,
            "n_filters_time": 40,
            "dropout": 0.1,
            "n_positions": 512,
            "lr_scheduler": "linear",
            "max_grad_norm": 1.0,
            "adam_beta_1": 0.9,
            "adam_beta_2": 0.999,
            "adam_epsilon": 1e-8,
            "weight_decay": 0.1,
            "optim": "adamw_hf",
            "warmup_ratio": 0.1,
        }

        os.makedirs(
            self.config["log_dir"],
            exist_ok=True
        )
        
        if self.config['set_seed']:
            random.seed(self.config["seed"])
            manual_seed(self.config["seed"])

    def fit(self, X: List[np.ndarray|List[BaseRaw]], y: List[np.ndarray|List[str]], meta: List[Dict]) -> None:
        print("inside fit NeuroGPT")

        task_name = meta[0]["task_name"]

        self.config["num_decoding_classes"] = n_unique_labels(task_name)

        train_dataset = NeuroGPTDataset2(X, y, meta, task_name, sample_keys=[
                    'inputs',
                    'attention_mask'
                ], chunk_len=self.config["chunk_len"], num_chunks=self.config["num_chunks"], ovlp=self.config["chunk_ovlp"], root_path="", gpt_only= not self.config["use_encoder"])

        val_split = 0.2
        if val_split is not None:
            train_dataset, val_dataset = train_dataset.split_train_val(val_split)
        else:
            val_dataset = None

        class_weights = torch.tensor(calc_class_weights(y, task_name))

        def model_init(params: Dict=None):
            model_config = dict(self.config)
            if params is not None:
                model_config |= params

            return make_model(model_config, class_weights)

        model_save_steps = self.config["training_steps"]*2


        self.trainer = make_trainer(
            model_init=model_init,
            training_style=self.config["training_style"],
            run_name=self.config["run_name"],
            output_dir=self.config["log_dir"],
            train_dataset=train_dataset,
            validation_dataset=val_dataset,
            per_device_train_batch_size=self.config["per_device_training_batch_size"],
            per_device_eval_batch_size=self.config["per_device_validation_batch_size"],
            dataloader_num_workers=self.config["num_workers"],
            optim=self.config["optim"],
            learning_rate=self.config["learning_rate"],
            weight_decay=self.config["weight_decay"],
            adam_beta1=self.config["adam_beta_1"],
            adam_beta2=self.config["adam_beta_1"],
            adam_epsilon=self.config["adam_epsilon"],
            max_grad_norm=self.config["max_grad_norm"],
            lr_scheduler_type=self.config["lr_scheduler"],
            warmup_ratio=self.config["warmup_ratio"],
            max_steps=self.config["training_steps"],
            # num_train_epochs=5,
            save_steps=model_save_steps,
            logging_steps=self.config["log_every_n_steps"],
            eval_steps=self.config["eval_every_n_steps"],
            seed=self.config["seed"] if self.config['set_seed'] else np.random.choice(range(1, 100000)),
            fp16=self.config["fp16"],
            deepspeed=self.config["deepspeed"],
        )

        self.trainer.train(resume_from_checkpoint=self.config["resume_from"])
        self.trainer.save_model(
            os.path.join(
                self.config["log_dir"],
                'model_final'
            )
        )

    def predict(self, X: List[np.ndarray|List[BaseRaw]], meta: List[Dict]) -> np.ndarray:
        print("inside predict")

        task_name = meta[0]["task_name"]

        self.test_dataset = NeuroGPTDataset2(X, None, meta, task_name, sample_keys=[
                    'inputs',
                    'attention_mask'
                ], chunk_len=self.config["chunk_len"], num_chunks=self.config["num_chunks"], ovlp=self.config["chunk_ovlp"], root_path="", gpt_only= not self.config["use_encoder"])

        test_prediction = self.trainer.predict(self.test_dataset)

        if test_prediction.label_ids is None:
            def softmax(logits):
                exp_logits = np.exp(logits - np.max(logits, axis=-1, keepdims=True))  # for numerical stability
                return exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)

            # Convert logits to probabilities
            probabilities = softmax(test_prediction.predictions)

            # Get the predicted classes (0 or 1) by taking argmax along axis 1 (the class axis)
            pred = np.argmax(probabilities, axis=1)
        else:
            pred = test_prediction.label_ids

        pred = np.array([reverse_map_label(p, task_name) for p in pred])

        return pred
        
